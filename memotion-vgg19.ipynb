{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f45fe86a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T09:25:04.400895Z",
     "iopub.status.busy": "2024-07-31T09:25:04.400039Z",
     "iopub.status.idle": "2024-07-31T09:25:13.068912Z",
     "shell.execute_reply": "2024-07-31T09:25:13.067917Z"
    },
    "papermill": {
     "duration": 8.677938,
     "end_time": "2024-07-31T09:25:13.071277",
     "exception": false,
     "start_time": "2024-07-31T09:25:04.393339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "from memotion_utility import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc7c6164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T09:25:13.082725Z",
     "iopub.status.busy": "2024-07-31T09:25:13.082221Z",
     "iopub.status.idle": "2024-07-31T09:25:13.087758Z",
     "shell.execute_reply": "2024-07-31T09:25:13.086910Z"
    },
    "papermill": {
     "duration": 0.013034,
     "end_time": "2024-07-31T09:25:13.089685",
     "exception": false,
     "start_time": "2024-07-31T09:25:13.076651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "CSV_FILE = '/kaggle/input/memotion-dataset-7k/memotion_dataset_7k/labels.csv'\n",
    "ROOT_DIR = '/kaggle/input/memotion-dataset-7k/memotion_dataset_7k/images'\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (224, 224)\n",
    "LR = 1e-3\n",
    "epochs = 20\n",
    "seed = 123\n",
    "downsample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eeb0bee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T09:25:13.100002Z",
     "iopub.status.busy": "2024-07-31T09:25:13.099738Z",
     "iopub.status.idle": "2024-07-31T09:25:13.221329Z",
     "shell.execute_reply": "2024-07-31T09:25:13.220422Z"
    },
    "papermill": {
     "duration": 0.129711,
     "end_time": "2024-07-31T09:25:13.224022",
     "exception": false,
     "start_time": "2024-07-31T09:25:13.094311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : \n",
      " label\n",
      "1    1953\n",
      "0    1953\n",
      "Name: count, dtype: int64\n",
      "val : \n",
      " label\n",
      "1    343\n",
      "0    217\n",
      "Name: count, dtype: int64\n",
      "test : \n",
      " label\n",
      "1    856\n",
      "0    543\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df,val_df,test_df = load_data(CSV_FILE,downsample = downsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1bcec4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T09:25:13.235159Z",
     "iopub.status.busy": "2024-07-31T09:25:13.234861Z",
     "iopub.status.idle": "2024-07-31T09:25:13.249845Z",
     "shell.execute_reply": "2024-07-31T09:25:13.248916Z"
    },
    "papermill": {
     "duration": 0.022753,
     "end_time": "2024-07-31T09:25:13.251786",
     "exception": false,
     "start_time": "2024-07-31T09:25:13.229033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>text</th>\n",
       "      <th>offensive</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>image_5658.png</td>\n",
       "      <td>LET NO ONE TELL YOU YOU ARE UNIMPORTANT</td>\n",
       "      <td>offensive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6636</th>\n",
       "      <td>image_5801.png</td>\n",
       "      <td>Facebook was a mistake.” -Mark Zuckerberg</td>\n",
       "      <td>offensive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4755</th>\n",
       "      <td>image_3098.jpg</td>\n",
       "      <td>socially amazing penguin's entry MemeCenter</td>\n",
       "      <td>offensive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>image_5695.jpg</td>\n",
       "      <td>Yo Chuck ima gonna let you finish but.... You ...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6465</th>\n",
       "      <td>image_5216.jpg</td>\n",
       "      <td>I'M JUST GONNA WATCH FINDING NEMO WITH MY SON ...</td>\n",
       "      <td>offensive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5518</th>\n",
       "      <td>image_5858.jpg</td>\n",
       "      <td>FRIENDS WHO GOSSIP ABOUT OTHERS TO YOU ARE GOS...</td>\n",
       "      <td>not offensive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3538</th>\n",
       "      <td>image_5036.jpg</td>\n",
       "      <td>SOME WOMEN PLAY MORE MIND GAMES THAN PROFESSOR...</td>\n",
       "      <td>not offensive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>image_6725.jpg</td>\n",
       "      <td>GO AHEAD. SAY \" SHUT UP  WESLEY!\" YOU BALD-  H...</td>\n",
       "      <td>not offensive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>image_6291.jpg</td>\n",
       "      <td>HEY GIRL GOD CALLED... YOU TO MINISTRY. imgfli...</td>\n",
       "      <td>not offensive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2853</th>\n",
       "      <td>image_2210.jpg</td>\n",
       "      <td>I BELIEVE WHATEVER DOESN'T KILL YOU SIMPLY MAK...</td>\n",
       "      <td>not offensive</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3906 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_name                                               text  \\\n",
       "427   image_5658.png            LET NO ONE TELL YOU YOU ARE UNIMPORTANT   \n",
       "6636  image_5801.png          Facebook was a mistake.” -Mark Zuckerberg   \n",
       "4755  image_3098.jpg        socially amazing penguin's entry MemeCenter   \n",
       "1034  image_5695.jpg  Yo Chuck ima gonna let you finish but.... You ...   \n",
       "6465  image_5216.jpg  I'M JUST GONNA WATCH FINDING NEMO WITH MY SON ...   \n",
       "...              ...                                                ...   \n",
       "5518  image_5858.jpg  FRIENDS WHO GOSSIP ABOUT OTHERS TO YOU ARE GOS...   \n",
       "3538  image_5036.jpg  SOME WOMEN PLAY MORE MIND GAMES THAN PROFESSOR...   \n",
       "2156  image_6725.jpg  GO AHEAD. SAY \" SHUT UP  WESLEY!\" YOU BALD-  H...   \n",
       "2732  image_6291.jpg  HEY GIRL GOD CALLED... YOU TO MINISTRY. imgfli...   \n",
       "2853  image_2210.jpg  I BELIEVE WHATEVER DOESN'T KILL YOU SIMPLY MAK...   \n",
       "\n",
       "          offensive  label  \n",
       "427       offensive      1  \n",
       "6636      offensive      1  \n",
       "4755      offensive      1  \n",
       "1034      offensive      1  \n",
       "6465      offensive      1  \n",
       "...             ...    ...  \n",
       "5518  not offensive      0  \n",
       "3538  not offensive      0  \n",
       "2156  not offensive      0  \n",
       "2732  not offensive      0  \n",
       "2853  not offensive      0  \n",
       "\n",
       "[3906 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fb160bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T09:25:13.263271Z",
     "iopub.status.busy": "2024-07-31T09:25:13.262991Z",
     "iopub.status.idle": "2024-07-31T09:25:13.270464Z",
     "shell.execute_reply": "2024-07-31T09:25:13.269612Z"
    },
    "papermill": {
     "duration": 0.015637,
     "end_time": "2024-07-31T09:25:13.272659",
     "exception": false,
     "start_time": "2024-07-31T09:25:13.257022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        self.df =  dataframe\n",
    "        #self.columns_to_map = ['humour', 'sarcasm', 'offensive', 'motivational']\n",
    "        #self.df[self.columns_to_map] = self.df[self.columns_to_map].applymap(self.map_values)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "#     @staticmethod   \n",
    "#     def map_values(value):\n",
    "#         if value.lower() in ['funny', 'very_funny', 'hilarious']:\n",
    "#             return 1\n",
    "#         elif value.lower() in ['general', 'twisted_meaning', 'very_twisted']:\n",
    "#             return 1\n",
    "#         elif value.lower() in ['slight', 'very_offensive', 'hateful_offensive']:\n",
    "#             return 1\n",
    "#         elif value.lower() == 'motivational':\n",
    "#             return 1\n",
    "#         else:\n",
    "#             return 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.df.iloc[idx]['image_name'])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        label = self.df.iloc[idx]['label']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e364aea0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T09:25:13.283831Z",
     "iopub.status.busy": "2024-07-31T09:25:13.283577Z",
     "iopub.status.idle": "2024-07-31T09:25:13.287586Z",
     "shell.execute_reply": "2024-07-31T09:25:13.286787Z"
    },
    "papermill": {
     "duration": 0.011684,
     "end_time": "2024-07-31T09:25:13.289419",
     "exception": false,
     "start_time": "2024-07-31T09:25:13.277735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def load_data(csv_file,transform):\n",
    "#     df = pd.read_csv(csv_file)\n",
    "#     train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "#     train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "    \n",
    "#     train_dataset = MemeDataset(train_df,ROOT_DIR,transform = transform)\n",
    "#     val_dataset = MemeDataset(val_df,ROOT_DIR,transform = transform)\n",
    "#     test_dataset = MemeDataset(test_df,ROOT_DIR,transform = transform)\n",
    "    \n",
    "#     train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "#     return train_loader,val_loader,test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e1adfce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T09:25:13.300428Z",
     "iopub.status.busy": "2024-07-31T09:25:13.300188Z",
     "iopub.status.idle": "2024-07-31T09:25:13.304802Z",
     "shell.execute_reply": "2024-07-31T09:25:13.303920Z"
    },
    "papermill": {
     "duration": 0.012331,
     "end_time": "2024-07-31T09:25:13.306780",
     "exception": false,
     "start_time": "2024-07-31T09:25:13.294449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "     transforms.Resize(IMAGE_SIZE),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07c135f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T09:25:13.317877Z",
     "iopub.status.busy": "2024-07-31T09:25:13.317609Z",
     "iopub.status.idle": "2024-07-31T09:25:13.323015Z",
     "shell.execute_reply": "2024-07-31T09:25:13.322245Z"
    },
    "papermill": {
     "duration": 0.013005,
     "end_time": "2024-07-31T09:25:13.324799",
     "exception": false,
     "start_time": "2024-07-31T09:25:13.311794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = MemeDataset(train_df,ROOT_DIR,transform = transform)\n",
    "val_dataset = MemeDataset(val_df,ROOT_DIR,transform = transform)\n",
    "test_dataset = MemeDataset(test_df,ROOT_DIR,transform = transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cc041ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T09:25:13.335755Z",
     "iopub.status.busy": "2024-07-31T09:25:13.335443Z",
     "iopub.status.idle": "2024-07-31T09:25:13.467542Z",
     "shell.execute_reply": "2024-07-31T09:25:13.466621Z"
    },
    "papermill": {
     "duration": 0.139977,
     "end_time": "2024-07-31T09:25:13.469696",
     "exception": false,
     "start_time": "2024-07-31T09:25:13.329719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-2.0837, -2.0837, -2.0837,  ..., -1.3130, -1.3130, -1.2959],\n",
       "          [-2.0837, -2.0837, -2.0837,  ..., -1.3130, -1.3130, -1.2788],\n",
       "          [-2.0837, -2.0837, -2.0837,  ..., -1.2959, -1.2959, -1.2788],\n",
       "          ...,\n",
       "          [-2.1008, -2.1008, -2.1008,  ..., -1.6898, -1.6555, -1.6727],\n",
       "          [-2.1008, -2.1008, -2.1008,  ..., -1.7069, -1.6555, -1.6898],\n",
       "          [-2.1008, -2.1008, -2.1008,  ..., -1.7583, -1.7240, -1.7754]],\n",
       " \n",
       "         [[-2.0007, -2.0007, -2.0007,  ..., -1.6331, -1.6331, -1.6331],\n",
       "          [-2.0007, -2.0007, -2.0007,  ..., -1.6155, -1.6155, -1.6155],\n",
       "          [-2.0007, -2.0007, -2.0007,  ..., -1.6155, -1.5980, -1.6155],\n",
       "          ...,\n",
       "          [-2.0182, -2.0182, -2.0182,  ..., -1.7031, -1.7206, -1.7206],\n",
       "          [-2.0182, -2.0182, -2.0182,  ..., -1.7206, -1.7206, -1.7381],\n",
       "          [-2.0182, -2.0182, -2.0182,  ..., -1.7031, -1.6856, -1.7381]],\n",
       " \n",
       "         [[-1.7347, -1.7347, -1.7347,  ..., -1.8044, -1.7870, -1.7522],\n",
       "          [-1.7347, -1.7347, -1.7347,  ..., -1.8044, -1.7696, -1.7522],\n",
       "          [-1.7347, -1.7347, -1.7347,  ..., -1.7870, -1.7522, -1.7522],\n",
       "          ...,\n",
       "          [-1.7522, -1.7522, -1.7522,  ..., -1.6650, -1.6650, -1.6476],\n",
       "          [-1.7522, -1.7522, -1.7522,  ..., -1.6999, -1.6650, -1.6650],\n",
       "          [-1.7696, -1.7696, -1.7696,  ..., -1.6476, -1.6127, -1.6476]]]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "699dc0c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T09:25:13.481448Z",
     "iopub.status.busy": "2024-07-31T09:25:13.481178Z",
     "iopub.status.idle": "2024-07-31T09:25:18.744770Z",
     "shell.execute_reply": "2024-07-31T09:25:18.743723Z"
    },
    "papermill": {
     "duration": 5.272325,
     "end_time": "2024-07-31T09:25:18.747528",
     "exception": false,
     "start_time": "2024-07-31T09:25:13.475203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [00:03<00:00, 170MB/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg19 = models.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3958567b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T09:25:18.771914Z",
     "iopub.status.busy": "2024-07-31T09:25:18.771031Z",
     "iopub.status.idle": "2024-07-31T09:25:18.777678Z",
     "shell.execute_reply": "2024-07-31T09:25:18.776585Z"
    },
    "papermill": {
     "duration": 0.020527,
     "end_time": "2024-07-31T09:25:18.779841",
     "exception": false,
     "start_time": "2024-07-31T09:25:18.759314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg19.classifier[6].in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58382ddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T09:25:18.798245Z",
     "iopub.status.busy": "2024-07-31T09:25:18.797743Z",
     "iopub.status.idle": "2024-07-31T09:25:19.104645Z",
     "shell.execute_reply": "2024-07-31T09:25:19.103831Z"
    },
    "papermill": {
     "duration": 0.318349,
     "end_time": "2024-07-31T09:25:19.107044",
     "exception": false,
     "start_time": "2024-07-31T09:25:18.788695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for param in vgg19.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the classifier part for binary classification\n",
    "vgg19.classifier[6] = nn.Linear(vgg19.classifier[6].in_features, 2)  # binary classification\n",
    "vgg19 = vgg19.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5aca60d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T09:25:19.125886Z",
     "iopub.status.busy": "2024-07-31T09:25:19.125154Z",
     "iopub.status.idle": "2024-07-31T09:42:57.510426Z",
     "shell.execute_reply": "2024-07-31T09:42:57.509233Z"
    },
    "papermill": {
     "duration": 1058.396981,
     "end_time": "2024-07-31T09:42:57.512723",
     "exception": false,
     "start_time": "2024-07-31T09:25:19.115742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 9/123 [00:06<01:13,  1.55it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [01:17<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.8113\n",
      "Accuracy: 46.61%\n",
      "Precision: 0.53\n",
      "Recall: 0.52\n",
      "F1 Score: 0.46\n",
      "Confusion Matrix:\n",
      "[[166  51]\n",
      " [248  95]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 11/123 [00:04<00:41,  2.67it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:44<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 0.7881\n",
      "Accuracy: 44.64%\n",
      "Precision: 0.50\n",
      "Recall: 0.50\n",
      "F1 Score: 0.44\n",
      "Confusion Matrix:\n",
      "[[158  59]\n",
      " [251  92]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 68/123 [00:24<00:19,  2.78it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:44<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Loss: 0.7833\n",
      "Accuracy: 57.32%\n",
      "Precision: 0.51\n",
      "Recall: 0.50\n",
      "F1 Score: 0.48\n",
      "Confusion Matrix:\n",
      "[[ 43 174]\n",
      " [ 65 278]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 67/123 [00:24<00:19,  2.86it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:44<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Loss: 0.7751\n",
      "Accuracy: 56.07%\n",
      "Precision: 0.49\n",
      "Recall: 0.49\n",
      "F1 Score: 0.47\n",
      "Confusion Matrix:\n",
      "[[ 41 176]\n",
      " [ 70 273]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 96/123 [00:35<00:10,  2.59it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:44<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 0.7771\n",
      "Accuracy: 47.86%\n",
      "Precision: 0.50\n",
      "Recall: 0.50\n",
      "F1 Score: 0.48\n",
      "Confusion Matrix:\n",
      "[[131  86]\n",
      " [206 137]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 45/123 [00:16<00:29,  2.67it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:44<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Loss: 0.7662\n",
      "Accuracy: 58.21%\n",
      "Precision: 0.52\n",
      "Recall: 0.51\n",
      "F1 Score: 0.48\n",
      "Confusion Matrix:\n",
      "[[ 41 176]\n",
      " [ 58 285]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 52/123 [00:18<00:25,  2.79it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:44<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Loss: 0.7662\n",
      "Accuracy: 53.21%\n",
      "Precision: 0.48\n",
      "Recall: 0.48\n",
      "F1 Score: 0.48\n",
      "Confusion Matrix:\n",
      "[[ 58 159]\n",
      " [103 240]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 96/123 [00:35<00:09,  2.82it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:44<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Loss: 0.7917\n",
      "Accuracy: 52.86%\n",
      "Precision: 0.50\n",
      "Recall: 0.50\n",
      "F1 Score: 0.50\n",
      "Confusion Matrix:\n",
      "[[ 81 136]\n",
      " [128 215]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 25/123 [00:09<00:35,  2.75it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:44<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Loss: 0.7709\n",
      "Accuracy: 52.32%\n",
      "Precision: 0.50\n",
      "Recall: 0.50\n",
      "F1 Score: 0.50\n",
      "Confusion Matrix:\n",
      "[[ 90 127]\n",
      " [140 203]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 89/123 [00:32<00:12,  2.72it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:44<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Loss: 0.7787\n",
      "Accuracy: 46.96%\n",
      "Precision: 0.50\n",
      "Recall: 0.50\n",
      "F1 Score: 0.47\n",
      "Confusion Matrix:\n",
      "[[140  77]\n",
      " [220 123]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 35/123 [00:12<00:34,  2.59it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:45<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Loss: 0.7705\n",
      "Accuracy: 42.14%\n",
      "Precision: 0.46\n",
      "Recall: 0.47\n",
      "F1 Score: 0.42\n",
      "Confusion Matrix:\n",
      "[[147  70]\n",
      " [254  89]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 90/123 [00:32<00:12,  2.69it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:44<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Loss: 0.7681\n",
      "Accuracy: 43.21%\n",
      "Precision: 0.48\n",
      "Recall: 0.48\n",
      "F1 Score: 0.42\n",
      "Confusion Matrix:\n",
      "[[153  64]\n",
      " [254  89]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 115/123 [00:41<00:02,  2.76it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:44<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Loss: 0.7754\n",
      "Accuracy: 53.75%\n",
      "Precision: 0.50\n",
      "Recall: 0.50\n",
      "F1 Score: 0.49\n",
      "Confusion Matrix:\n",
      "[[ 69 148]\n",
      " [111 232]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 55/123 [00:19<00:24,  2.79it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:44<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Loss: 0.7669\n",
      "Accuracy: 52.86%\n",
      "Precision: 0.50\n",
      "Recall: 0.50\n",
      "F1 Score: 0.50\n",
      "Confusion Matrix:\n",
      "[[ 85 132]\n",
      " [132 211]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 19/123 [00:06<00:36,  2.83it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:44<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Loss: 0.7776\n",
      "Accuracy: 48.04%\n",
      "Precision: 0.51\n",
      "Recall: 0.51\n",
      "F1 Score: 0.48\n",
      "Confusion Matrix:\n",
      "[[134  83]\n",
      " [208 135]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 12/123 [00:04<00:39,  2.79it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:44<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Loss: 0.7778\n",
      "Accuracy: 48.04%\n",
      "Precision: 0.50\n",
      "Recall: 0.50\n",
      "F1 Score: 0.48\n",
      "Confusion Matrix:\n",
      "[[130  87]\n",
      " [204 139]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 25/123 [00:09<00:34,  2.81it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:44<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Loss: 0.8044\n",
      "Accuracy: 57.32%\n",
      "Precision: 0.52\n",
      "Recall: 0.51\n",
      "F1 Score: 0.50\n",
      "Confusion Matrix:\n",
      "[[ 54 163]\n",
      " [ 76 267]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 35/123 [00:13<00:32,  2.72it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:45<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Loss: 0.7979\n",
      "Accuracy: 50.18%\n",
      "Precision: 0.52\n",
      "Recall: 0.52\n",
      "F1 Score: 0.50\n",
      "Confusion Matrix:\n",
      "[[129  88]\n",
      " [191 152]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 47/123 [00:17<00:27,  2.74it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:44<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Loss: 0.8094\n",
      "Accuracy: 52.50%\n",
      "Precision: 0.52\n",
      "Recall: 0.52\n",
      "F1 Score: 0.52\n",
      "Confusion Matrix:\n",
      "[[112 105]\n",
      " [161 182]]\n",
      "Classification Report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 65/123 [00:23<00:22,  2.62it/s]/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 123/123 [00:44<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 0.8017\n",
      "Accuracy: 47.14%\n",
      "Precision: 0.48\n",
      "Recall: 0.47\n",
      "F1 Score: 0.47\n",
      "Confusion Matrix:\n",
      "[[105 112]\n",
      " [184 159]]\n",
      "Classification Report:\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(vgg19.classifier[6].parameters(), lr=LR)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    vgg19.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = vgg19(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    \n",
    "    vgg19.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = vgg19(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='macro')\n",
    "    recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "    class_report = classification_report(all_labels, all_predictions, target_names=['Not Offensive', 'Offensive'])\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"Classification Report:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67750941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T09:42:57.922273Z",
     "iopub.status.busy": "2024-07-31T09:42:57.921659Z",
     "iopub.status.idle": "2024-07-31T09:43:26.944281Z",
     "shell.execute_reply": "2024-07-31T09:43:26.943030Z"
    },
    "papermill": {
     "duration": 29.230085,
     "end_time": "2024-07-31T09:43:26.946292",
     "exception": false,
     "start_time": "2024-07-31T09:42:57.716207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.68%\n",
      "Precision: 0.4970\n",
      "Recall: 0.4969\n",
      "F1 Score: 0.4905\n",
      "Confusion Matrix:\n",
      "[[270 273]\n",
      " [431 425]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Offensive       0.39      0.50      0.43       543\n",
      "    Offensive       0.61      0.50      0.55       856\n",
      "\n",
      "     accuracy                           0.50      1399\n",
      "    macro avg       0.50      0.50      0.49      1399\n",
      " weighted avg       0.52      0.50      0.50      1399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the model and compute metrics\n",
    "vgg19.eval()\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = vgg19(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions, average='macro')\n",
    "recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "class_report = classification_report(all_labels, all_predictions, target_names=['Not Offensive', 'Offensive'])\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06d6b05",
   "metadata": {
    "papermill": {
     "duration": 0.203474,
     "end_time": "2024-07-31T09:43:27.353788",
     "exception": false,
     "start_time": "2024-07-31T09:43:27.150314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 531544,
     "sourceId": 973292,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 190469921,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1107.685721,
   "end_time": "2024-07-31T09:43:29.282198",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-31T09:25:01.596477",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
